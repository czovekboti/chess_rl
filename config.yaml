mistral_config:
  project: "Chess_RL_Project"
  name: "Mistral_model"
  model: "mistralai/Mistral-7B-Instruct-v0.2"
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 3.5e-5
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.05
  warmup_ratio: 0.15
  max_seq_length: 1024
  lora_rank: 32
  save_steps: 50
  max_steps: 1000

llama_config:
  project: "Chess_RL_Project"
  name: "LLaMA3_model"
  model: "unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit"
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 3e-5
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.05
  warmup_ratio: 0.15
  max_seq_length: 1024
  lora_rank: 32
  save_steps: 50
  max_steps: 1000

PHI_config:
  project: "Chess_RL_Project"
  name: "PHI"
  model: "unsloth/Phi-4"
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 1e-5
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.05
  max_model_len: 512
  warmup_ratio: 0.15
  max_seq_length: 1024
  lora_rank: 32
  save_steps: 50
  max_steps: 1000
  
qwen4b_config:
  project: "Chess_RL_Project"
  name: "qwen-4B"
  model: "unsloth/Qwen3-4B-Instruct-2507"
  per_device_train_batch_size: 6
  gradient_accumulation_steps: 2
  learning_rate: 1e-5
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.05
  max_model_len: 512
  warmup_ratio: 0.15
  max_seq_length: 1024
  lora_rank: 32
  save_steps: 50
  max_steps: 1000

qwen7b_config:
  project: "Chess_RL_Project"
  name: "qwen-7B"
  model: "unsloth/Qwen2.5-Coder-7B-Instruct"
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  learning_rate: 1e-5
  adam_beta1: 0.9
  adam_beta2: 0.99
  weight_decay: 0.05
  max_model_len: 512
  warmup_ratio: 0.15
  max_seq_length: 1024
  lora_rank: 32
  save_steps: 50
  max_steps: 1000
  

