{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ULOMxDUzuDIh",
      "metadata": {
        "id": "ULOMxDUzuDIh"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "  https://colab.research.google.com/github/czovekboti/chess_rl/blob/sft%2Bgrpo/blob/main/SFT%20Trainer%20notebook.ipynb\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TzPH4w3Pu-8l",
      "metadata": {
        "id": "TzPH4w3Pu-8l"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GtzoS2XmwBRK",
      "metadata": {
        "id": "GtzoS2XmwBRK"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "!pip install python-chess\n",
        "!apt-get install stockfish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VKx2WOcYvAln",
      "metadata": {
        "id": "VKx2WOcYvAln"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e09c882",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e09c882",
        "outputId": "e3c5bab7-6291-4ef6-adf2-74ffa8677c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected config_name: qwen4b\n",
            "STOCKFISH_PATH: /usr/games/stockfish\n"
          ]
        }
      ],
      "source": [
        "#@title Load model config{ display-mode: \"form\" }\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import yaml\n",
        "\n",
        "# Path to your YAML config file\n",
        "path = '/content/config.yaml'\n",
        "\n",
        "def load_config(path: str):\n",
        "    with open(path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "config = load_config(path)\n",
        "config_name = \"qwen4b\"\n",
        "print(\"Selected config_name:\", config_name)\n",
        "\n",
        "match config_name:\n",
        "    case \"llama\":\n",
        "        config = config[\"llama_config\"]\n",
        "    case \"phi\":\n",
        "        config = config[\"PHI_config\"]\n",
        "    case \"mistral\":\n",
        "        config = config[\"mistral_config\"]\n",
        "    case \"qwen7b\":\n",
        "        config = config[\"qwen7b_config\"]\n",
        "    case \"qwen4b\":\n",
        "        config = config[\"qwen4b_config\"]\n",
        "    case _:\n",
        "        raise ValueError(\"Check model name â€“ perhaps the keyboard got excited.\")\n",
        "\n",
        "# Stockfish path from env\n",
        "# stockfish_path = os.getenv(\"STOCKFISH_PATH\")\n",
        "stockfish_path= '/usr/games/stockfish'\n",
        "\n",
        "print(\"STOCKFISH_PATH:\", stockfish_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PP8kW-8KxNOG",
      "metadata": {
        "id": "PP8kW-8KxNOG"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pC3DhkywFWG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "1c02eb45774d44e9a29775e29504a4f8",
            "e6a64eba532c41298d6bbd9976e9881f",
            "7afc8351b1304f728ea1acfe1ec4a87b",
            "71867e114f804c019689aae03c2c38b1",
            "a64b1a07470d49ff8e05cc198d8e6ebd",
            "8382a4fd680e4aef84882cc595bc6ae1",
            "cc8f371071fc483e87184601968a93c5",
            "8634e87a84314cb9b83d465449da96a9",
            "77c8108fa81a445c8e20f81aa9897475",
            "22072c5799334ffa8040060d736f1b05",
            "b7457bdb45de4286a3f612805ed88902"
          ]
        },
        "id": "8pC3DhkywFWG",
        "outputId": "f9356518-5419-418b-b7ed-0999ee9879fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14547\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c02eb45774d44e9a29775e29504a4f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16108\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset,concatenate_datasets\n",
        "dataset = load_dataset(\"czovekboti/sft_training2\")\n",
        "print(len(dataset[\"train\"]))\n",
        "\n",
        "json_dataset = load_dataset(\"json\", data_files=\"training.json\")\n",
        "\n",
        "# Concatenate the 'train' splits from both datasets\n",
        "dataset[\"train\"] = concatenate_datasets([\n",
        "    dataset[\"train\"],\n",
        "    json_dataset[\"train\"]\n",
        "])\n",
        "print(len(dataset[\"train\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qnTmQNcnxLdH",
      "metadata": {
        "id": "qnTmQNcnxLdH"
      },
      "source": [
        "# Load model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d2f225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "e55ec688686a43dda9674b1026abba16",
            "4d47fda27149426496c2c26825777499",
            "5dde7c725937441b93b7f98e67ad98af",
            "26dca8089b1e45589e278de518490552",
            "63c8a832d8f843b6b2df7b62b15db096",
            "583900bbfba64decae6d41ebde2ea7b8",
            "8ddb5cd0f5fa452ab75094ec5c797d75",
            "543fd741c1ea4c66a9d3720647fd44f0",
            "2b3b0804dc7d446781a340df03966adc",
            "586b84677bbc4bdd829da54ed1cb5a98",
            "20f4b28ae6674ecd941453de04fb55f3",
            "0bb5a3d4874446858eb8370cbd48ecb3",
            "b051c8b8a8094b15a8b8a9b0694b1749",
            "0b8ad07f67e149f6977c52e9bbbb213d",
            "f4805386100c424093712fb789ad9ab5",
            "b54e6123eb1447fb92dc81a6f2fac898",
            "e40557c0737a48bd8858017d249e1792",
            "c7b808375e2d4980a92708e71c862065",
            "8052b6cf2b3b412cbad3ba768eda693c",
            "f77588ab2a8749328a8abf14c575bbf3",
            "879d5c80ca7c44858ede829109a712c2",
            "7bc9021edab94ff1a4caa69b9ee1f9ce",
            "54dfd8f86e7c4cbca3d28fb0197a73ef",
            "af19a6761a424164a61c1c9d17f928a5",
            "3723c6ab1e4a4161b4d94e400d8e4cca",
            "6cd16e8792474dd7bbaa6820ccb61c27",
            "17bfd942fd164cb4b1266bf1553fbb85",
            "ec835894606c49c5b88046271bd8d9a0",
            "9c4daa665d2d4764a57a02beeae9e9ab",
            "822e85b3d97c4c40a1ee463a183acf69",
            "c9c8bf6b5c72459c8d80a372e9c10c6c",
            "c65c6231b1c6405daf1ab50917c5a254",
            "1ce0497055f24ea481ef5265e4a62641",
            "54ed3b3f042c473fba912a23cf4de6fa",
            "89a5188f073b4dfca30948e22b142dc5",
            "80f5cda613b84c6a9b1cb965b6f6464e",
            "31b0ade459c548a5bc072d8cc25f4d11",
            "7db9d02e950b447a8aa85a827a881891",
            "8abc79cc4f75435fa8f93e07da8210fb",
            "d6a06f32e828453fa6404ba94f44cbec",
            "cb99b989de4e4b4486307d7ff30e0866",
            "bf76f78fb4a34662a48539f45cad668f",
            "94f1e595a28d44c691e84cc745994ffc",
            "6a3ea79608f44b0eb41f5a111144026e",
            "5e23bb641652479ca9941e191ad67e25",
            "bdc3b1a380b341b7a395c262d62c25f4",
            "00aa1d9ee70c40e7b0de24f84f9b0ae2",
            "5753544eba064721984744a0d0eb6cce",
            "09b6e5639b7e440da8f5d1f42002ceee",
            "6c417fad26764bd58245583ff02c2fbf",
            "8b6b092ad01348feae9494b11fec0e1b",
            "4a3ef2736c1041158175d31d3d779c4d",
            "7e7ed7aa5f0e459dbe8e99983075f308",
            "315412bbc3844f76ab7acec797194364",
            "337ef82a0a8f42c48c9f159b48f1a81e",
            "6af02514f14244b7b6719d74e204453a",
            "8bfd4853bbb94c52beabb6450db90fd9",
            "dc865606f10b42bbb0eb9302263af833",
            "09d09ca08e4144cdb3a2d2cf123b7938",
            "0e421e270ec74f209cabb8519605d8ee",
            "6ce33fdbcf2444d19366d92dfdab0d07",
            "9def307b16ae46969d1f28256ae70efc",
            "18dda4e1749c47a0936d1a60742405ff",
            "719abc50fdbe41a083401bbac4a672a8",
            "b9b33f81b24a42bba3a26a41630fc5c2",
            "7fb5329afe2e407aa15f4496fa6cf99f",
            "1b9cf059f45244849544bfe0d5f3efc7",
            "16135f3d9bf045cca1c126f46586f70b",
            "a8e33509a1b74e4b92406f085255d5f6",
            "d2581b9c14b7421bb18e35305291456f",
            "ce9c6ddb09d7430381f197925a1ca3af",
            "93771965ba044b399b95a7673f728789",
            "4ff0e1a7436a4efb8bf455152e6af5d5",
            "55bb754e9ac7401c8dc50e770e5380c3",
            "076320247a4447708986b9a9261c38ab",
            "7a2daf163be248819a9bc105d4289d55",
            "acc9bddbc9ad474d8c8827fe69f5dfcd",
            "2509c5c823244de1a01822dd4403bb6f",
            "73e36fb9b05249fea1058e62c5c7520f",
            "23499445589b4ee890b26b07df33d146",
            "46b7ec4e92604219b60414358a07172f",
            "55a9d11d4c00457988065d4a6b9384f5",
            "e7b9fb7e188746a2915b0a709b3dda5c",
            "78ddce5fd79845d9a0a521de862afa2c",
            "254dacb1774d47dd9cf07986e9e2dfd1",
            "3f1c6d826c56469e81947c268d22c4cb",
            "2977127487ff47e8b7dc953e4c077874",
            "2c5f882972824954b4f968f9ef3c0ce7",
            "b5b689c20b4248c488a14fc1cc90375e",
            "514cea32c7f8464298c678e438167d1f",
            "8a5a5c179d744c84a141e8cffaa2dacd",
            "7cc6a237545d4c7abd948390f018f0ae",
            "a391de39f61a49ccac86b8bb78277210",
            "603b053e889e4e088a8ed065e0c946cd",
            "6246bc2db35f43dc87b9cc432e880364",
            "7ae5cb749a9348be8f8db465c94c0a22",
            "95b6a70a960c4c38ab74110c626ee225",
            "1f7e9f36146c48c0871d075356a1faca",
            "56176b8cc5dd4a3dafd806c84d154412",
            "04cc48b6909c44c990a1b1ca7f2043ad",
            "b1ac38d6997145fab5bada1ad611490d",
            "c2baad3acb16407e838583ad968f67e5",
            "e09570f9a9744024bf8663a1a75f8bf6",
            "9b6b060c14e74226bf7ac043f4e4f299",
            "5abca4ee4a0544f7b00f43575bceca4d",
            "409d13aee13b46ce8f1515ff0cdf0a80",
            "85262fc868274d9dbd32356015d91843",
            "cb456905c393444e96d1d88c5fac1ca8",
            "f7953d8632f94e0daa5955bc894fc085",
            "3f70740f990f435fbd312965b5a6c346",
            "93068a9814744009891c3668cc35218e",
            "87ac3691cc5d429d93541e358f7d327f",
            "f1a488d3833f499ea2d2a5d8e7b9aabc",
            "fe7ee58ee69a40cc9425a87dfdc0e436",
            "7c7e905ef9f24b0cb5a302f383d8da82",
            "dd4913d762c945beb294bf00f026ff0d",
            "23df96941a5f46adb1a6deefffd624f7",
            "69159da9c67f480f8eb1b834ab0be1ef",
            "8eead1328b2b47beb6f54034e2ccda4f",
            "1ced3b0f4d0b42e983f4dc3bfd87cdfc",
            "0345e912be9a45fe8f6303a978984502",
            "78e1334496df474eb9ac0cd0bb5d4391",
            "9d43986768c04234941e3e5adfd6e6fd",
            "d8ceb87c33e8434da98ed04dc7aca218",
            "0e70d9672a614a9480a637f6ed26c50e",
            "0305ee27e20d481dbaee3e0ebe346b54",
            "44a9262bdf3a482ab45f6f4a2e593dd2",
            "a5544fbc697a492baee758f0547acd1d",
            "df6d979722d44f0aa76799202fba93bf",
            "8dc6ac1791ad43dd8f7f61aee20994ff",
            "1f0fd20786274c10ba394817a81e2723",
            "774631f519eb47c9999dc9919eed7196",
            "a4ba5d5af4b0489492454dd047d3379e",
            "6b2abdd77a684799bba8a819ca892063",
            "9b84bf05b12741f6a729283757a13df4",
            "f2aa0eb5215f4e69a270c513a1eacfde",
            "5eb2f01ad8584ff5800a1de2e0e15cca",
            "8ca1f54dde6d4b12bcbfd522e1edd989",
            "5cf7ca3ff24541119f7d99e1b739a5da",
            "af69691b03044d369c0312a3b56a3277",
            "f77c24fc566447e7ad21a7d083883f23",
            "d41c58ec85c2404a877a68f18f83a102",
            "070cea75d58543f48e59295ea58be980"
          ]
        },
        "id": "85d2f225",
        "outputId": "4ded1e7a-ab0a-400f-9236-65924d6925dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 11-12 15:41:19 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 11-12 15:41:21 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e55ec688686a43dda9674b1026abba16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bb5a3d4874446858eb8370cbd48ecb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54dfd8f86e7c4cbca3d28fb0197a73ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54ed3b3f042c473fba912a23cf4de6fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e23bb641652479ca9941e191ad67e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6af02514f14244b7b6719d74e204453a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b9cf059f45244849544bfe0d5f3efc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2509c5c823244de1a01822dd4403bb6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5b689c20b4248c488a14fc1cc90375e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04cc48b6909c44c990a1b1ca7f2043ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93068a9814744009891c3668cc35218e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78e1334496df474eb9ac0cd0bb5d4391",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4ba5d5af4b0489492454dd047d3379e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.2 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = config[\"model\"],\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = False, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
        ")\n",
        "#add paddings\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token or \"<|pad|>\"\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "target_modules = [\n",
        "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "    \"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "\n",
        "]\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_rank, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "znIbAhMZw8E1",
      "metadata": {
        "id": "znIbAhMZw8E1"
      },
      "source": [
        "# Prompt, data preparation and model test\n",
        "model test is commented out run for before-after comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sLSQFdRrdoW_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLSQFdRrdoW_",
        "outputId": "93ee875d-a268-4928-e100-0befea029a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['fen', 'top_5_moves', 'answer'],\n",
            "    num_rows: 14497\n",
            "})\n",
            "Dataset({\n",
            "    features: ['fen', 'top_5_moves', 'answer'],\n",
            "    num_rows: 1611\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "# original_dataset = load_dataset(\"czovekboti/sft_chess\", split=\"train\")\n",
        "\n",
        "split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = split[\"train\"]\n",
        "test_ds = split[\"test\"]\n",
        "print(train_ds)\n",
        "print(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7OITtBp54joN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OITtBp54joN",
        "outputId": "e4b51665-82bf-48ba-cde4-7a9dc82ffb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING MODEL BEFORE TRAINING + SCORING\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\n",
        "\n",
        "Please follow this exact format in your response:\n",
        "\n",
        "<reasoning>\n",
        "(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\n",
        "</reasoning>\n",
        "<answer>\n",
        "(best move written in correct SAN format, such as Nf3 or exd5)\n",
        "</answer>\n",
        "\n",
        "Do not invent illegal or impossible moves. The move must be legal in the given FEN position.\n",
        "Do not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\n",
        "In case of taking a piece use the [file]x[target square] format\n",
        "### Example:\n",
        "FEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\n",
        "\n",
        "<reasoning>\n",
        "White has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\n",
        "</reasoning>\n",
        "<answer>\n",
        "d5\n",
        "</answer>\n",
        "\n",
        "Now solve the following position:\n",
        "\"\"\"\n",
        "\n",
        "def fen_color(fen: str) -> str:\n",
        "    try:\n",
        "        parts = fen.strip().split()\n",
        "        side = parts[1]\n",
        "        return \"White\" if side == \"w\" else \"Black\" if side == \"b\" else \"Unknown\"\n",
        "    except Exception:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# ---------- add this block after loading model/tokenizer ----------\n",
        "import torch, re, pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# (optional but recommended) ensure a chat template is set for base models\n",
        "try:\n",
        "    from unsloth.chat_templates import get_chat_template\n",
        "    if getattr(tokenizer, \"chat_template\", None) in (None, \"\"):\n",
        "        tokenizer = get_chat_template(tokenizer, chat_template=\"qwen3\")\n",
        "except Exception:\n",
        "    # fallback: minimal qwen-like template\n",
        "    if getattr(tokenizer, \"chat_template\", None) in (None, \"\"):\n",
        "        tokenizer.chat_template = (\n",
        "            \"{% for m in messages %}\"\n",
        "            \"{% if m['role'] == 'system' %}<|im_start|>system\\n{{ m['content'] }}<|im_end|>\\n\"\n",
        "            \"{% elif m['role'] == 'user' %}<|im_start|>user\\n{{ m['content'] }}<|im_end|>\\n\"\n",
        "            \"{% elif m['role'] == 'assistant' %}<|im_start|>assistant\\n{{ m['content'] }}<|im_end|>\\n\"\n",
        "            \"{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "def extract_move(text: str):\n",
        "    m = ANSWER_RE.search(text or \"\")\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def to_top5_set(v):\n",
        "    if v is None:\n",
        "        return set()\n",
        "    if isinstance(v, (list, tuple, set)):\n",
        "        return set(str(x).strip() for x in v if str(x).strip())\n",
        "    parts = [p.strip() for p in str(v).replace(\";\", \",\").split(\",\")]\n",
        "    return set(p for p in parts if p)\n",
        "\n",
        "def build_prompt(fen: str, color: str):\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": f\"FEN: {fen}\\nYou are with the following pieces: {color}\"},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "def generate_answer(the_model, prompt: str, max_new_tokens=128, temperature=0.7, top_p=0.9, top_k=50):\n",
        "    the_model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "        out = the_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=temperature > 0,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    gen_ids = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING MODEL BEFORE TRAINING + SCORING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "ANSWER_RE = re.compile(r\"<answer>\\s*(.*?)\\s*</answer>\", re.DOTALL | re.IGNORECASE)\n",
        "REASONING_RE = re.compile(r\"<reasoning>\\s*(.*?)\\s*</reasoning>\", re.DOTALL | re.IGNORECASE)\n",
        "DEVICE = next(model.parameters()).device if 'model' in globals() else (\n",
        "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "def test_model(train_ds, model,answer_re,reasoning_re, max = len(train_ds)):\n",
        "  i=0\n",
        "  rows = []\n",
        "  for example in train_ds:\n",
        "        if i== max:\n",
        "          return rows\n",
        "        fen = example[\"fen\"]\n",
        "        color = fen_color(fen)\n",
        "        prompt = build_prompt(fen, color)\n",
        "        output = generate_answer(model, prompt, max_new_tokens=128, temperature=0.0, top_p=1.0, top_k=0)  # deterministic\n",
        "\n",
        "        move = extract_move(output)\n",
        "        has_reasoning = bool(REASONING_RE.search(output))\n",
        "        has_answer_tag = bool(ANSWER_RE.search(output))\n",
        "\n",
        "        top5 = to_top5_set(example.get(\"top_5_moves\"))\n",
        "        in_top5 = (move in top5) if move else False\n",
        "\n",
        "        best_move = (example.get(\"best_move\") or \"\").strip() or None\n",
        "        equals_best = (move == best_move) if (move and best_move) else False\n",
        "\n",
        "        score = 0\n",
        "        score += 1 if has_reasoning else 0\n",
        "        score += 1 if has_answer_tag else 0\n",
        "        score += 1 if move else 0\n",
        "        score += 2 if in_top5 else 0\n",
        "        score += 2 if equals_best else 0\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Example {i+1}/{len(train_ds)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"FEN: {fen}\")\n",
        "        print(f\"Color: {color}\")\n",
        "        print(f\"\\nGenerated Answer:\\n{output}\")\n",
        "\n",
        "        rows.append({\n",
        "            \"idx\": i,\n",
        "            \"FEN\": fen,\n",
        "            \"Color\": color,\n",
        "            \"Move\": move or \"\",\n",
        "            \"HasReasoningTag\": has_reasoning,\n",
        "            \"HasAnswerTag\": has_answer_tag,\n",
        "            \"InTop5\": in_top5,\n",
        "            \"EqualsBest\": equals_best,\n",
        "            \"Score\": score,\n",
        "            \"Top5Moves\": \", \".join(sorted(top5)) if top5 else \"\",\n",
        "            \"BestMove\": best_move or \"\",\n",
        "            \"RawOutput\": output,\n",
        "        })\n",
        "        i+=1\n",
        "  return rows\n",
        "# MODEL TEST =====\n",
        "# rows = test_model(test_ds, model,ANSWER_RE,REASONING_RE,10)\n",
        "# df = pd.DataFrame(rows).sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
        "# print(\"\\n=== SCORE TABLE (top 10 rows) ===\")\n",
        "# print(df[[\"idx\",\"Move\",\"InTop5\",\"EqualsBest\",\"HasReasoningTag\",\"HasAnswerTag\",\"Score\"]].head(50))\n",
        "\n",
        "# # save\n",
        "# df.to_csv(\"sft_answer_scoring_before_training.csv\", index=False)\n",
        "# print(\"\\nSaved: sft_answer_scoring_before_training.csv\")\n",
        "# print(\"\\nStarting training...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_dBTamRYxS24",
      "metadata": {
        "id": "_dBTamRYxS24"
      },
      "source": [
        "# WANDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b0dc80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "35b0dc80",
        "outputId": "e25970cd-7528-4146-ce75-8d94bb725758"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mczovekboti\u001b[0m (\u001b[33mczovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251112_155636-jfxn7wdr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/jfxn7wdr' target=\"_blank\">QWEN4B last sft test run</a></strong> to <a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project' target=\"_blank\">https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/jfxn7wdr' target=\"_blank\">https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/jfxn7wdr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B initialized.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Toggle W&B logging (set to False if you don't want to log)\n",
        "USE_WANDB = True\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
        "    os.environ[\"WANDB_PROJECT\"] = \"Chess_RL_Project\"\n",
        "    os.environ[\"WANDB_ENTITY\"] = \"czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem\"\n",
        "    wandb.login()\n",
        "    wandb.init(\n",
        "        project=\"Chess_RL_Project\",\n",
        "        entity=\"\",\n",
        "        name= \"QWEN4B last sft test run\",\n",
        "        config={\n",
        "            \"model\": config[\"model\"],\n",
        "            \"max_seq_length\": config['max_seq_length'],\n",
        "            \"lora_rank\": lora_rank,\n",
        "            \"learning_rate\": config[\"learning_rate\"],\n",
        "            \"max_steps\": config[\"max_steps\"],\n",
        "        }\n",
        "    )\n",
        "    print(\"W&B initialized.\")\n",
        "else:\n",
        "    print(\"W&B disabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BbYE1y7kxWcM",
      "metadata": {
        "id": "BbYE1y7kxWcM"
      },
      "source": [
        "# Formatting dataset to match gpro training input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_XFJVedZAVyH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "e2252d7abbb343c1ba8d31f7e0943752",
            "7c87984507154945958238a30899c487",
            "f2b217776f6a48168f2cc8bde02a4f48",
            "73f47267c2554175ad5f83f06b51c225",
            "26ba8316dd9549f9a0dd479fdf16420f",
            "9409f9d63123484681e42af541a9fe72",
            "b59219cfc1f54a1cb49e51da43a8ff52",
            "e7835a09d674447db75c4d67ea3b490e",
            "1d84126b07e74b1cbf9d8b3be96d801e",
            "cf673ba0281d4d209be55b88d7875ed5",
            "c7c5104c57d1443ca4673dfa0d0057ca",
            "fdfae0a7c4d24895a74771d67e24927d",
            "52cb61e0ef5d4614870cff1e079b2913",
            "ad8c511595c84c759f3b0c657ed9f983",
            "9ca231248dfc4d61bcf80577459257be",
            "d6c9844c5429432f88922b82484eb385",
            "c3c61dd78a61485298cf42fe7e198a7e",
            "6c833034589a4f1abe4b9231c83e29d9",
            "87ea9cd16c38428991504d52759c4df2",
            "a4c57d324c964573b27c260d75f043a8",
            "ed3d3dda77dc4d3f8b9acfc29a986be5",
            "48de0aefc25d4f2b81106823ad4a507e"
          ]
        },
        "id": "_XFJVedZAVyH",
        "outputId": "5d88a79e-88e4-4da8-8dc7-42f508f1b5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['fen', 'top_5_moves', 'answer'],\n",
            "    num_rows: 14497\n",
            "})\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2252d7abbb343c1ba8d31f7e0943752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/14497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'content': 'You are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\\n\\nPlease follow this exact format in your response:\\n\\n<reasoning>\\n(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\\n</reasoning>\\n<answer>\\n(best move written in correct SAN format, such as Nf3 or exd5)\\n</answer>\\n\\nDo not invent illegal or impossible moves. The move must be legal in the given FEN position.\\nDo not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\\nIn case of taking a piece use the [file]x[target square] format\\n### Example:\\nFEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\\n\\n<reasoning>\\nWhite has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\\n</reasoning>\\n<answer>\\nd5\\n</answer>\\n\\nNow solve the following position:\\n', 'role': 'system'}, {'content': '\\nFEN: 1rr4k/p3b1pp/1p2Nnp1/1B2p3/P5P1/2PP3P/1P3P2/2R2R1K b - - 5 24\\nYou are with the following pieces: Black', 'role': 'user'}, {'content': \"<reasoning>\\nh6 weakens black's kingside position and prepares to develop the knight to a strong square.\\ng6 is a safe move but doesn't directly address the threats to the king.\\nf6 is a backward pawn move that weakens the center and opens up the queen for black.\\ne6 is a passive move that doesn't counter the threats to the king or develop pieces effectively.\\nTherefore, h6 is best because it directly addresses the immediate threats to the king and prepares to develop the knight to a strong square.\\n</reasoning>\\n<answer>\\nh6\\n</answer>\", 'role': 'assistant'}], 'text': \"<|im_start|>system\\nYou are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\\n\\nPlease follow this exact format in your response:\\n\\n<reasoning>\\n(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\\n</reasoning>\\n<answer>\\n(best move written in correct SAN format, such as Nf3 or exd5)\\n</answer>\\n\\nDo not invent illegal or impossible moves. The move must be legal in the given FEN position.\\nDo not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\\nIn case of taking a piece use the [file]x[target square] format\\n### Example:\\nFEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\\n\\n<reasoning>\\nWhite has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\\n</reasoning>\\n<answer>\\nd5\\n</answer>\\n\\nNow solve the following position:\\n<|im_end|>\\n<|im_start|>user\\n\\nFEN: 1rr4k/p3b1pp/1p2Nnp1/1B2p3/P5P1/2PP3P/1P3P2/2R2R1K b - - 5 24\\nYou are with the following pieces: Black<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n<reasoning>\\nh6 weakens black's kingside position and prepares to develop the knight to a strong square.\\ng6 is a safe move but doesn't directly address the threats to the king.\\nf6 is a backward pawn move that weakens the center and opens up the queen for black.\\ne6 is a passive move that doesn't counter the threats to the king or develop pieces effectively.\\nTherefore, h6 is best because it directly addresses the immediate threats to the king and prepares to develop the knight to a strong square.\\n</reasoning>\\n<answer>\\nh6\\n</answer><|im_end|>\\n\"}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdfae0a7c4d24895a74771d67e24927d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1611 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'content': 'You are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\\n\\nPlease follow this exact format in your response:\\n\\n<reasoning>\\n(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\\n</reasoning>\\n<answer>\\n(best move written in correct SAN format, such as Nf3 or exd5)\\n</answer>\\n\\nDo not invent illegal or impossible moves. The move must be legal in the given FEN position.\\nDo not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\\nIn case of taking a piece use the [file]x[target square] format\\n### Example:\\nFEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\\n\\n<reasoning>\\nWhite has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\\n</reasoning>\\n<answer>\\nd5\\n</answer>\\n\\nNow solve the following position:\\n', 'role': 'system'}, {'content': '\\nFEN: 1rbq1rk1/p3nppp/1p2pb2/3n4/3PB2P/P1N2N2/1P3PP1/R1BQR1K1 w - - 0 14\\nYou are with the following pieces: White', 'role': 'user'}, {'content': \"<reasoning>\\ng5 is a knight move that recaptures on the h-file, preventing White from recapturing with a queen or bishop and thus preserving the queen's mobility. The move also supports the development of the bishop to g5, which is a strong square for it. Additionally, it threatens to recapture on h4 with the queen, putting pressure on Black's position.\\n\\nNg5 is a knight move that advances the knight to the center, supporting the development of other pieces and controlling key squares. It also threatens to recapture on f6 with the queen, putting pressure on Black's position. However, it does not directly prevent White from recapturing with a queen or bishop and thus does not preserve the queen's mobility as effectively as g5.\\n\\nTherefore, g5 is objectively the best move because it directly prevents White from recapturing with a queen or bishop while also supporting the development of other pieces and controlling key squares.\\n</reasoning>\\n<answer>\\ng5\\n</answer>\", 'role': 'assistant'}], 'text': \"<|im_start|>system\\nYou are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\\n\\nPlease follow this exact format in your response:\\n\\n<reasoning>\\n(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\\n</reasoning>\\n<answer>\\n(best move written in correct SAN format, such as Nf3 or exd5)\\n</answer>\\n\\nDo not invent illegal or impossible moves. The move must be legal in the given FEN position.\\nDo not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\\nIn case of taking a piece use the [file]x[target square] format\\n### Example:\\nFEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\\n\\n<reasoning>\\nWhite has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\\n</reasoning>\\n<answer>\\nd5\\n</answer>\\n\\nNow solve the following position:\\n<|im_end|>\\n<|im_start|>user\\n\\nFEN: 1rbq1rk1/p3nppp/1p2pb2/3n4/3PB2P/P1N2N2/1P3PP1/R1BQR1K1 w - - 0 14\\nYou are with the following pieces: White<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n<reasoning>\\ng5 is a knight move that recaptures on the h-file, preventing White from recapturing with a queen or bishop and thus preserving the queen's mobility. The move also supports the development of the bishop to g5, which is a strong square for it. Additionally, it threatens to recapture on h4 with the queen, putting pressure on Black's position.\\n\\nNg5 is a knight move that advances the knight to the center, supporting the development of other pieces and controlling key squares. It also threatens to recapture on f6 with the queen, putting pressure on Black's position. However, it does not directly prevent White from recapturing with a queen or bishop and thus does not preserve the queen's mobility as effectively as g5.\\n\\nTherefore, g5 is objectively the best move because it directly prevents White from recapturing with a queen or bishop while also supporting the development of other pieces and controlling key squares.\\n</reasoning>\\n<answer>\\ng5\\n</answer><|im_end|>\\n\"}\n",
            "14497\n",
            "1611\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "print(train_ds)\n",
        "def format_dataset(ds):\n",
        "    formatted_texts = []\n",
        "    for ex in ds:\n",
        "        fen = (ex.get(\"fen\") or \"\").strip()\n",
        "        ans = (ex.get(\"answer\") or \"\").strip()\n",
        "        if not fen or not ans:\n",
        "            continue\n",
        "        color = fen_color(fen)\n",
        "        messages = [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"\\nFEN: {fen}\\nYou are with the following pieces: {color}\"},\n",
        "                {\"role\": \"assistant\", \"content\": ans},\n",
        "\n",
        "        ]\n",
        "        formatted_texts.append({\"conversations\": messages})\n",
        "    dataset = Dataset.from_list(formatted_texts)\n",
        "    dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "    return dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "final_train_ds = format_dataset(train_ds)\n",
        "print(final_train_ds[0])\n",
        "final_test_ds = format_dataset(test_ds)\n",
        "print(final_test_ds[0])\n",
        "print(len(final_train_ds))\n",
        "print(len(final_test_ds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ravi-4xcBi",
      "metadata": {
        "id": "e4ravi-4xcBi"
      },
      "source": [
        "# TRAIN\n",
        "\n",
        "Trains on whole dataset, saves every 1000th step.\n",
        "Uses validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xglo-8sbCsVN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "08a127ec2d9342b38842c8fcaa3b4729",
            "8543b7e4903044978f7ec44b46684701",
            "551f263f2d6448cf95e9d084eac8d9d7",
            "69c839d3709344c3a4674b863aff7a1f",
            "35166f23b13748f3964a4f791bac490c",
            "59d17a031e5d4ce5977b328541d9c4c9",
            "ad8f19392cf648a5b8032db363188561",
            "a8ebddaa6e934693af22162eaaabece0",
            "d8d324381d2e4c7b813a658aaafe71f5",
            "2edec9951f2a41efa3a4da7d26e03fdf",
            "86ab595bddf14882b281b7296185e06c",
            "2c6ce8754fcd4fd19fe31938d620e712",
            "fa183deb4a264ac2a771818cfb81690e",
            "411f006ea8184a4ba2e62522129a87e6",
            "5508727a573849558e528b75b72f22e3",
            "f6d3e25f8f9544f8a845a94edcd77b71",
            "b87f4551badb4e7ab7c0f7897fda612e",
            "9c21164d8e1e4ee6bfe1aa965083b40c",
            "378cdb265a3f4234b96862d7915423b8",
            "cc82418dd9b34018840151361aef6cd2",
            "ace6774c484a40cba48d27bf45f59352",
            "f7f2652fb1d34dc1a1b90e55fbcfa804"
          ]
        },
        "id": "xglo-8sbCsVN",
        "outputId": "9882c0c5-3eba-4cbf-9339-126745703a51"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08a127ec2d9342b38842c8fcaa3b4729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/14497 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c6ce8754fcd4fd19fe31938d620e712",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/1611 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = final_train_ds,\n",
        "    eval_dataset=final_test_ds,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        completion_only_loss=True,\n",
        "        per_device_train_batch_size = 2,\n",
        "        per_device_eval_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = len(final_train_ds)+ len(final_test_ds),\n",
        "        metric_for_best_model = \"eval_loss\",\n",
        "        eval_strategy=\"steps\",   # evaluate every N steps\n",
        "        eval_steps=10,                 # <-- evaluate every 10 steps\n",
        "        save_strategy=\"steps\",        # <-- save every 10 steps\n",
        "        save_steps= 1000,\n",
        "        save_total_limit = 3,\n",
        "        learning_rate = 1e-3,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"./sft_outputs\",\n",
        "        report_to = \"wandb\", # Use TrackIO/WandB\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v-Se6vP6uVrw",
      "metadata": {
        "id": "v-Se6vP6uVrw"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1X2CHTQrxwrZ",
      "metadata": {
        "id": "1X2CHTQrxwrZ"
      },
      "source": [
        "# Test model answers after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2jPbVeCg8CiR",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2jPbVeCg8CiR"
      },
      "outputs": [],
      "source": [
        "trained_model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WAu6cH1z8K7b",
      "metadata": {
        "id": "WAu6cH1z8K7b"
      },
      "outputs": [],
      "source": [
        "rows  = test_model(test_ds,trained_model,ANSWER_RE,REASONING_RE)\n",
        "df = pd.DataFrame(rows).sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
        "print(\"\\n=== SCORE TABLE (top 10 rows) ===\")\n",
        "print(df[[\"idx\",\"Move\",\"InTop5\",\"EqualsBest\",\"HasReasoningTag\",\"HasAnswerTag\",\"Score\"]].head(50))\n",
        "df.to_csv(\"sft_answer_scoring_after_training.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aO7_Rs69x0qG",
      "metadata": {
        "id": "aO7_Rs69x0qG"
      },
      "source": [
        "If ran test before training comparison of legal moves is possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0cFLp4s_S79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0cFLp4s_S79",
        "outputId": "60b1331e-5ff6-46b3-ca6f-05442d398d8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of legal moves: 0\n",
            "Number of legal moves: 8\n"
          ]
        }
      ],
      "source": [
        "import chess\n",
        "import pandas as pd\n",
        "\n",
        "# simple legality check\n",
        "def is_legal_move(fen, san):\n",
        "    try:\n",
        "        board = chess.Board(fen)\n",
        "        move = board.parse_san(san)  # will raise error if move invalid\n",
        "        return move in board.legal_moves\n",
        "    except Exception:\n",
        "        return False\n",
        "before_df = pd.read_csv(\"sft_answer_scoring_before_training.csv\")\n",
        "before_df[\"is_legal\"] = before_df.apply(lambda x: is_legal_move(x[\"FEN\"], x[\"Move\"]), axis=1)\n",
        "after_df = pd.read_csv(\"sft_answer_scoring_after_training.csv\")\n",
        "after_df[\"is_legal\"] = after_df.apply(lambda x: is_legal_move(x[\"FEN\"], x[\"Move\"]), axis=1)\n",
        "legal_count = before_df[\"is_legal\"].sum()\n",
        "print(\"Number of legal moves:\", legal_count)\n",
        "legal_count = after_df[\"is_legal\"].sum()\n",
        "print(\"Number of legal moves:\", legal_count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
