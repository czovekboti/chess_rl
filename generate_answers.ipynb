{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3aGcfMYP9B66",
      "metadata": {
        "id": "3aGcfMYP9B66"
      },
      "source": [
        "# gen_with_unsloth_top5.ipynb\n",
        "Local Unsloth-based generator that saves Top-5 engine candidates + XML answer per sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bx57t5SdAT2H",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bx57t5SdAT2H"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install chess\n",
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3lEdlOtl9B68",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3lEdlOtl9B68"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "# Generate chess SFT data with Unsloth and SAVE Top-5 candidates per sample.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "import argparse\n",
        "import chess\n",
        "import chess.pgn\n",
        "import chess.engine\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zTq-gizkBilN",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zTq-gizkBilN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get install stockfish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0mRt3se_yhp",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T0mRt3se_yhp"
      },
      "outputs": [],
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "from datasets import load_dataset, Dataset\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import torch , gc\n",
        "\n",
        "# Configuration\n",
        "engine_path = \"/usr/games/stockfish\"  # Update this path\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a chess expert using reasoning. Use <reasoning> </reasoning> for your reasoning and <answer> </answer> tags.\"\n",
        ")\n",
        "\n",
        "def generate_prompt(fen, candidates, best_move):\n",
        "  candidates = candidates[1:3] if len(candidates) > 0 else candidates[0]\n",
        "  prompt = f\"\"\"Analyze the following chess position and determine the single best move.\n",
        "\n",
        "For each candidate move, give one concise sentence explaining its key strength or weakness.\n",
        "Then, in 1–2 sentences, justify which move is objectively the best and why it is superior.\n",
        "\n",
        "Position (FEN): {fen}\n",
        "Candidate moves (SAN): {candidates}\n",
        "Best move: {best_move}\n",
        "\n",
        "Respond strictly in this XML format:\n",
        "<reasoning>\n",
        "[Concise comparison and explanation of all moves, followed by why the best move is superior.]\n",
        "</reasoning>\n",
        "<answer>\n",
        "[Best move in SAN notation]\n",
        "</answer>\n",
        "\n",
        "Rules:\n",
        "- Only include the provided candidate moves in your reasoning.\n",
        "- The <answer> tag must contain exactly one SAN move — the best move.\n",
        "- Do NOT include any text outside the XML tags.\n",
        "\n",
        "Example:\n",
        "<reasoning>\n",
        "d4 controls the center and opens lines for the light-squared bishop.\n",
        "Nc3 develops a piece but doesn't contest the center as directly.\n",
        "Nf3 is solid but less ambitious. h3 and c3 are slow moves that fail to fight for space.\n",
        "Therefore, d4 is best because it develops control and supports classical central strategy.\n",
        "</reasoning>\n",
        "<answer>\n",
        "d4\n",
        "</answer>\n",
        "  \"\"\"\n",
        "  return prompt\n",
        "\n",
        "def get_best_moves(fen: str, engine_path: str, num_moves: int = 5) -> List[Dict]:\n",
        "    \"\"\"Get top N moves from Stockfish analysis.\"\"\"\n",
        "    board = chess.Board(fen)\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(engine_path)\n",
        "\n",
        "    try:\n",
        "        best_moves = engine.analyse(\n",
        "            board,\n",
        "            chess.engine.Limit(depth=18),\n",
        "            multipv=num_moves\n",
        "        )\n",
        "        moves_data = []\n",
        "        for info in best_moves:\n",
        "          move = info[\"pv\"][0]\n",
        "          san_move = board.san(move)\n",
        "          moves_data.append(san_move)\n",
        "\n",
        "        return moves_data\n",
        "    finally:\n",
        "        engine.quit()\n",
        "def load_model():\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Qwen2.5-Coder-7B-Instruct\",\n",
        "        max_seq_length=2048,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    return model, tokenizer\n",
        "def generate_answer(model, tokenizer, prompt: str,max_retries=3) -> str:\n",
        "    for attempt in range(max_retries):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.7 + (attempt * 0.1),  # Increase temp on retries\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        generated = tokenizer.decode(\n",
        "            outputs[0][inputs['input_ids'].shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        answer = generated.strip()\n",
        "\n",
        "        # Check if tags exist\n",
        "        has_reasoning = \"<reasoning>\" in answer.lower() and \"</reasoning>\" in answer.lower()\n",
        "        has_answer = \"<answer>\" in answer.lower() and \"</answer>\" in answer.lower()\n",
        "\n",
        "        if has_reasoning and has_answer:\n",
        "            print(f\"✓ Valid answer generated (attempt {attempt + 1})\")\n",
        "            print(answer)\n",
        "            return answer\n",
        "        else:\n",
        "            print(f\"✗ Invalid answer on attempt {attempt + 1} - missing tags\")\n",
        "            if not has_reasoning:\n",
        "                print(\"  Missing: <reasoning> tags\")\n",
        "            if not has_answer:\n",
        "                print(\"  Missing: <answer> tags\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  Retrying...\")\n",
        "            else:\n",
        "                print(f\"  Failed after {max_retries} attempts. Returning anyway.\")\n",
        "                print(answer)\n",
        "                return answer\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MsaWkOPfJIap",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a06130f76d0142f9944a14fe878c23a3",
            "2473894f425140c08d6142cae4c9955f"
          ]
        },
        "id": "MsaWkOPfJIap",
        "outputId": "369231b7-84ef-4b2a-b982-ae509ef33611"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06130f76d0142f9944a14fe878c23a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chessData.csv:   0%|          | 0.00/795M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2473894f425140c08d6142cae4c9955f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/12958035 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'FEN': 'rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1',\n",
              " 'Evaluation': '-10'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"czovekboti/chessdata\", split=\"train\")\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dkhyJMoaJzQU",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkhyJMoaJzQU",
        "outputId": "8a8726ea-556a-4134-853f-5092f4b3cb0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "#gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xRxbn-XOMacs",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xRxbn-XOMacs"
      },
      "outputs": [],
      "source": [
        "def process_dataset(dataset, engine_path: str, model, tokenizer, max_examples: int = None):\n",
        "    \"\"\"Process dataset: get moves, create prompts, generate answers.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Limit dataset if needed\n",
        "    dataset_slice = dataset if max_examples is None else dataset.select(range(min(max_examples, len(dataset))))\n",
        "\n",
        "    for idx, data in enumerate(dataset_slice):\n",
        "        try:\n",
        "            fen = data[\"FEN\"]\n",
        "            fen = fen[0] if isinstance(fen, list) else fen\n",
        "            top_5_moves = get_best_moves(fen, engine_path, num_moves=5)\n",
        "\n",
        "            if not top_5_moves or len(top_5_moves) == 0:\n",
        "                continue\n",
        "\n",
        "            best_move = top_5_moves[0]\n",
        "            candidates = \", \".join(top_5_moves)\n",
        "\n",
        "            prompt = generate_prompt(fen, candidates, best_move)\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Example {idx}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            print(prompt)\n",
        "            print(f\"{'='*60}\\n\")\n",
        "\n",
        "\n",
        "            answer = generate_answer(model, tokenizer, prompt)\n",
        "            results.append({\n",
        "                \"fen\": fen,\n",
        "                \"top_5_moves\": top_5_moves,\n",
        "                \"answer\": answer,\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError at example {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "riKEsZeIFxRp",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "riKEsZeIFxRp",
        "outputId": "c09d3165-b047-4133-ed59-c313618ec07f"
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-802055114.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"chess_training_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"Save results to both HuggingFace dataset and JSON.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Save as HuggingFace dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
          ]
        }
      ],
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "def save_results(results: List[Dict], output_name: str = \"chess_training_data\"):\n",
        "    \"\"\"Save results to both HuggingFace dataset and JSON.\"\"\"\n",
        "    # Save as HuggingFace dataset\n",
        "    dataset = Dataset.from_list(results)\n",
        "    dataset.save_to_disk(f\"./{output_name}\")\n",
        "\n",
        "    # Save as JSON for easy viewing\n",
        "    with open(f\"{output_name}.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Saved {len(results)} examples to {output_name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"CHESS TRAINING DATA GENERATOR\")\n",
        "    print(\"=\"*60)\n",
        "    # Load model\n",
        "    print(\"\\n[1/4] Loading AI model...\")\n",
        "    model, tokenizer = load_model()\n",
        "    # Load dataset\n",
        "    print(\"\\n[2/4] Loading chess dataset...\")\n",
        "    dataset = load_dataset(\"czovekboti/chessdata\", split=\"train\")\n",
        "    print(f\"Loaded {len(dataset)} positions\")\n",
        "\n",
        "    # Process dataset\n",
        "    print(\"\\n[3/4] Generating training data...\")\n",
        "    results = process_dataset(\n",
        "        dataset,\n",
        "        engine_path,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        #============================================================\n",
        "        max_examples=None  # < num of board examples\n",
        "        #===========================================================\n",
        "    )\n",
        "\n",
        "    # Save final results\n",
        "    print(\"\\n[4/4] Saving final results...\")\n",
        "    save_results(results, \"chess_training_data_final\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"COMPLETE! Generated {len(results)} training examples\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Display example\n",
        "    if results:\n",
        "        print(\"\\nEXAMPLE OUTPUT:\")\n",
        "        print(\"-\"*60)\n",
        "        print(f\"FEN: {results[0]['fen']}\")\n",
        "        print(f\"Top 5 Moves: {results[0]['top_5_moves']}\")\n",
        "        print(f\"\\nPrompt:\\n{results[0]['prompt'][:200]}...\")\n",
        "        print(f\"\\nAnswer:\\n{results[0]['answer']}\")\n",
        "        print(\"-\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
