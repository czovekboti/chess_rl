{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UUKs6e7Rcr1h"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bbVdn6ufc7tl"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "!pip install python-chess\n",
        "!apt-get install stockfish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hh7Vrm2Td17w"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QsuijSWyTtRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc035d3-f902-495d-c84c-a45b8a1662ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Connect drive{ display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Config Files\n",
        "- Model parameters are loaded from config\n",
        "- Stockfish has to be downloaded already, add path in env file"
      ],
      "metadata": {
        "id": "wolmWZSzRYGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jriYUdJNcLQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7227767a-90b0-4a2d-982d-9075674a6ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected config_name: qwen4b\n",
            "STOCKFISH_PATH: /usr/games/stockfish\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import yaml\n",
        "\n",
        "# Path to your YAML config file\n",
        "path = '/content/config.yaml'\n",
        "\n",
        "def load_config(path: str):\n",
        "    with open(path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "    return config\n",
        "\n",
        "config = load_config(path)\n",
        "config_name = \"qwen4b\"\n",
        "print(\"Selected config_name:\", config_name)\n",
        "\n",
        "match config_name:\n",
        "    case \"llama\":\n",
        "        config = config[\"llama_config\"]\n",
        "    case \"phi\":\n",
        "        config = config[\"PHI_config\"]\n",
        "    case \"mistral\":\n",
        "        config = config[\"mistral_config\"]\n",
        "    case \"qwen7b\":\n",
        "        config = config[\"qwen7b_config\"]\n",
        "    case \"qwen4b\":\n",
        "        config = config[\"qwen4b_config\"]\n",
        "    case _:\n",
        "        raise ValueError(\"Check model name â€“ perhaps the keyboard got excited.\")\n",
        "\n",
        "# Stockfish path from env\n",
        "# stockfish_path = os.getenv(\"STOCKFISH_PATH\")\n",
        "stockfish_path= '/usr/games/stockfish'\n",
        "\n",
        "print(\"STOCKFISH_PATH:\", stockfish_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model\n",
        "- Adding pad tokens so its compatible with sft trained lora adapters"
      ],
      "metadata": {
        "id": "qz9A-13xR1Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = config['max_seq_length']# Can increase for longer reasoning traces\n",
        "lora_rank = config['lora_rank'] # Larger rank = smarter, but slower\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = config[\"model\"],\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = False, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
        ")\n",
        "#add paddings\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token or \"<|pad|>\" # needed for compatibility with lora\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n"
      ],
      "metadata": {
        "id": "HIhQZFKmRiME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "9397748ba0214e3d8457571880a931d6",
            "24f19c64486e471399961073f6896368",
            "eb07609dfad649a2b7a860124a652ad9",
            "463da6390f414230affebc67dcee8379",
            "a5a5481be71f477797dce376440a295e",
            "399752160dbd486780f3fe54c7427a8a",
            "4f14cfaf25d74c1b9b99c82042e10f8a",
            "1aaff51d7dce43dabaca636c769bf10c",
            "28075cf6c24f457cb1a5bb079c302f5c",
            "0c028a1db48e42d496a7280e2a5a57e8",
            "4074d36091314dc7ab7c1b221851583e"
          ]
        },
        "outputId": "ba741f8b-905d-4a08-8908-32148a3579d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 11-12 22:16:06 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 11-12 22:16:08 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9397748ba0214e3d8457571880a931d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(151669, 2560, padding_idx=151654)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Lora\n",
        "- Change adapter dir as neccessary"
      ],
      "metadata": {
        "id": "m6oBROk1SEi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "lora_path = \"./sft_outputs/checkpoint-xy\"\n",
        "# model = FastLanguageModel.get_peft_model(\n",
        "#     model,\n",
        "#     r = lora_rank,\n",
        "#     target_modules = [\n",
        "#         \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "#         \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "#     ],\n",
        "#     lora_alpha = lora_rank,\n",
        "#     use_gradient_checkpointing = \"unsloth\",\n",
        "#     random_state = 3407,\n",
        "# )\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    lora_path,\n",
        "    is_trainable=True,\n",
        "    adapter_name=\"sft_adapter\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "HkLlUrvc1HV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717aa7cb-588a-4197-8182-21eb2ef956ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.32.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.32.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.33.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.33.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.34.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.34.mlp.down_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.q_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.q_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.k_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.k_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.v_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.v_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.o_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.self_attn.o_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.mlp.gate_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.mlp.gate_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.mlp.up_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.mlp.up_proj.lora_B.sft_adapter.weight', 'base_model.model.model.layers.35.mlp.down_proj.lora_A.sft_adapter.weight', 'base_model.model.model.layers.35.mlp.down_proj.lora_B.sft_adapter.weight'].\n",
            "  warnings.warn(warn_message)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tRd7SnKGf6E1",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "27329483-473a-4235-c1f0-0456e3fabef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mczovekboti\u001b[0m (\u001b[33mczovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251112_221730-i4p4c041</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/i4p4c041' target=\"_blank\">qwen-4B</a></strong> to <a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project' target=\"_blank\">https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/i4p4c041' target=\"_blank\">https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/i4p4c041</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/Chess_RL_Project/runs/i4p4c041?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7aab90fbbce0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#@title Wandb Setup{ display-mode: \"form\" }\n",
        "# Initialize wandb\n",
        "import wandb\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
        "os.environ[\"WANDB_PROJECT\"] = \"Chess_RL_Project\"\n",
        "os.environ[\"WANDB_ENTITY\"] = \"czovekboti-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem\"\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"Chess_RL_Project\",\n",
        "    entity = \"\",\n",
        "    name=config[\"name\"],\n",
        "    config={\n",
        "        \"model\": config[\"model\"],\n",
        "        \"max_seq_length\": config['max_seq_length'],\n",
        "        \"lora_rank\": lora_rank,\n",
        "        \"learning_rate\": config[\"learning_rate\"],\n",
        "        \"max_steps\": config[\"max_steps\"],\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HF2tvK5vpCbK"
      },
      "outputs": [],
      "source": [
        "#@title Load dataset{ display-mode: \"form\" }\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"czovekboti/chessdata\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training functions and prompt\n",
        "- Prompt:\n",
        "    - Gives instructions to the model alongside with examples\n",
        "    - Same as SFT training prompt\n",
        "- Functions:\n",
        "  - Basic functions for extracting answer and checking existance of reasoning tags\n",
        "  - correctness_reward_func: Loads board than checks if the move by the model is syntactically correct and valid. If yes always positive reward +/- the scaled evaluation given by stockfish. If the answer is incorrect negative reward is given."
      ],
      "metadata": {
        "id": "zPMCs3AFSew1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\n",
        "\n",
        "Please follow this exact format in your response:\n",
        "\n",
        "<reasoning>\n",
        "(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\n",
        "</reasoning>\n",
        "<answer>\n",
        "(best move written in correct SAN format, such as Nf3 or exd5)\n",
        "</answer>\n",
        "\n",
        "Do not invent illegal or impossible moves. The move must be legal in the given FEN position.\n",
        "Do not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\n",
        "In case of taking a piece use the [file]x[target square] format\n",
        "### Example:\n",
        "FEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\n",
        "\n",
        "<reasoning>\n",
        "White has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\n",
        "</reasoning>\n",
        "<answer>\n",
        "d5\n",
        "</answer>\n",
        "\n",
        "Now solve the following position:\n",
        "\"\"\"\n",
        "\n",
        "# import chess libaries and load engine\n",
        "import chess, chess.engine\n",
        "from chess import InvalidMoveError, IllegalMoveError, AmbiguousMoveError\n",
        "import math\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "# Load and prep dataset\n",
        "\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "def get_board(data, split = \"train\"):\n",
        "    def fen_color(fen: str) -> str:\n",
        "        return \"White\" if fen.split()[1] == 'w' else \"Black\"\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['FEN'] + \" You are with the following pieces: \" + fen_color(x['FEN'])}\n",
        "        ], 'evaluation': x['Evaluation'], 'fen': x['FEN']\n",
        "    }, remove_columns=data.column_names)\n",
        "    print(data[0])\n",
        "    return data #\n",
        "\n",
        "\n",
        "dataset = get_board(dataset.select(range(2000)))\n",
        "def reward_move(board, dataeval):\n",
        "  result = engine.analyse(board, chess.engine.Limit(time=1.0)) # time doesn't make a real difference above this\n",
        "  evaluation = result['score'].relative.score() #evaluation from opponents point of view\n",
        "  print(f\"\\n----------------------\\n\")\n",
        "  if evaluation is not None:\n",
        "      scaled_evaluation = math.tanh(evaluation / 900.0) * 2.0 # biggest eval for position in file is around 15000 but 2000+ evals are rare\n",
        "      if -evaluation > dataeval: # give reward if it improved position (-evaluation cause we need other players pov)\n",
        "        scaled_evaluation -= 0.5 # -0.5 because the sign is going to be flipped\n",
        "        print(f\"Eval = {-evaluation}, Dataeval = {dataeval}. State was improved->reward = 0.5\")\n",
        "      print(f\"Scaled Evaluation: {-scaled_evaluation} \")\n",
        "      return -scaled_evaluation # *-1 because we need the score of the player who is not in turn\n",
        "  else:\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "# Reward functions\n",
        "def correctness_reward_func(prompts,fen, completions, evaluation,**kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_moves =  [extract_xml_answer(r) for r in responses]\n",
        "    fen_str = fen[0] if isinstance(fen, list) else fen\n",
        "    board = chess.Board(fen_str)\n",
        "    print(f\"------------\\nFEN: {fen}\\n--------- \\nResponse: {responses[0]} \\n----------\\nExtracted_Move: {extracted_moves[0]}\")\n",
        "    rewards = []\n",
        "    try:\n",
        "        if isinstance(evaluation, list):\n",
        "            evaluation = float(evaluation[0]) # evaluation maybe a list due to a bug\n",
        "    except (ValueError, TypeError) as e:\n",
        "        print(f\"Error: Could not convert evaluation '{evaluation}' to float. Using default value 0.0.\")\n",
        "        evaluation = 0.0\n",
        "    # This also checks if the move is right both syntactically and legally\n",
        "    for move in extracted_moves:\n",
        "        try:\n",
        "          board.push_san(move)\n",
        "          scaled_evaluation = reward_move(board,evaluation) #evaluate board after the move was made\n",
        "          rewards.append(3.0+scaled_evaluation) # +5\n",
        "        except InvalidMoveError:\n",
        "            print(f\"\\n----------------------\\n-1.0 reward for illegal syntax\")\n",
        "            rewards.append(-5.0)\n",
        "        except ValueError:\n",
        "            print(f\"\\n----------------------\\n -0.7 reward for illegal move\")\n",
        "            rewards.append(-3.0)\n",
        "        except AmbiguousMoveError: #meaning two pieces could go to the declared square\n",
        "            print(f\"\\n----------------------\\n 0.5 reward for right syntax but ambigous move\")\n",
        "            rewards.append(0.5)\n",
        "    return rewards\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\s*.+?\\s*</reasoning>\\s*<answer>\\s*.+?\\s*</answer>\\s*$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r,re.DOTALL) for r in responses]\n",
        "    return [0.2 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, re.DOTALL) for r in responses]\n",
        "    return [0.2 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9En0OGmSdoj",
        "outputId": "41848a69-49aa-4654-cc5f-5ec7bc55ef39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': [{'content': '\\nYou are a chess coach assistant. You will be given a board position in FEN format. Your job is to analyze the board and suggest the best legal move for the player whose turn it is.\\n\\nPlease follow this exact format in your response:\\n\\n<reasoning>\\n(Brief explanation of what you see on the board â€” piece activity, threats, and candidate moves)\\n</reasoning>\\n<answer>\\n(best move written in correct SAN format, such as Nf3 or exd5)\\n</answer>\\n\\nDo not invent illegal or impossible moves. The move must be legal in the given FEN position.\\nDo not use UCI format like e2e4 â€” only SAN notation like e4, Nf3, or O-O.\\nIn case of taking a piece use the [file]x[target square] format\\n### Example:\\nFEN: rnbqkbnr/pppppppp/8/8/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 1\\n\\n<reasoning>\\nWhite has just played e4 and developed the knight to f3. Itâ€™s Blackâ€™s turn. The e4 pawn is undefended. Capturing it with the pawn from d7 to d5 is a natural central counter.\\n</reasoning>\\n<answer>\\nd5\\n</answer>\\n\\nNow solve the following position:\\n', 'role': 'system'}, {'content': 'rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1 You are with the following pieces: Black', 'role': 'user'}], 'evaluation': '-10', 'fen': 'rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "6ozZdgdfXOno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hdwoRjV_jWSb",
        "outputId": "d37272c8-99a6-42e8-d84d-67ffa2b3049e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 10\n",
            "O^O/ \\_/ \\    Batch size per device = 6 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (6 x 2 x 1) = 12\n",
            " \"-____-\"     Trainable parameters = 66,060,288 of 4,087,844,864 (1.62% trained)\n",
            "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 262144, 'temperature': 0.7, 'top_p': 0.8}. If this is not desired, please set these values explicitly.\n",
            "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:941: UserWarning: An output with one or more elements was resized since it had shape [1, 12, 2560], which does not match the required output shape [12, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
            "  out = torch_matmul(X, W.t(), out = out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "FEN: ['rnbqkbnr/pp2pppp/8/3p4/3P4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4', 'rnbqkbnr/pp2pppp/8/3p4/3P4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4', 'rnbqkbnr/pp2pppp/8/3p4/3P4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4', 'rnbqkbnr/pp2pppp/8/3p4/3P4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4', 'rnbqkbnr/pp2pppp/8/3p4/3P4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4', 'rnbqkbnr/pp2pppp/8/3p4/3P4/8/PPP2PPP/RNBQKBNR w KQkq - 0 4', '8/5pk1/R5p1/4Kn1p/7P/6P1/8/8 w - - 3 44', '8/5pk1/R5p1/4Kn1p/7P/6P1/8/8 w - - 3 44', '8/5pk1/R5p1/4Kn1p/7P/6P1/8/8 w - - 3 44', '8/5pk1/R5p1/4Kn1p/7P/6P1/8/8 w - - 3 44', '8/5pk1/R5p1/4Kn1p/7P/6P1/8/8 w - - 3 44', '8/5pk1/R5p1/4Kn1p/7P/6P1/8/8 w - - 3 44']\n",
            "--------- \n",
            "Response: <reasoning>\n",
            "The position shows a standard opening setup with white to play. The pawns are mostly centralized, with black having a strong center presence due to their three pawns on c6, d6, and e6 (implied by \"3p4\" on the fourth rank). White's light-squared bishop is active, but there seems to be some imbalance in development. However, notice that White has a potential initiative through king-side developmentâ€”specifically, the queen can support a push at e4 if needed. But more importantly, the main idea here is often to develop knights first and control the center without overextending.\n",
            "\n",
            "Looking closely, the key point is that Black has a solid structure and no immediate threat. White needs to initiate development while preparing for possible central expansion. A common early move in such positions is developing the knight to f3 or g3, but also consider controlling the center via e4. Since both sides have full rook files open, playing e4 would challenge Black's center and allow further coordination.\n",
            "\n",
            "However, note that after 4...cxd5? is not forced yet; rather, we should focus on safe and sound development. In many such setups, Nf3 is preferred because it develops a piece, \n",
            "----------\n",
            "Extracted_Move: <reasoning>\n",
            "The position shows a standard opening setup with white to play. The pawns are mostly centralized, with black having a strong center presence due to their three pawns on c6, d6, and e6 (implied by \"3p4\" on the fourth rank). White's light-squared bishop is active, but there seems to be some imbalance in development. However, notice that White has a potential initiative through king-side developmentâ€”specifically, the queen can support a push at e4 if needed. But more importantly, the main idea here is often to develop knights first and control the center without overextending.\n",
            "\n",
            "Looking closely, the key point is that Black has a solid structure and no immediate threat. White needs to initiate development while preparing for possible central expansion. A common early move in such positions is developing the knight to f3 or g3, but also consider controlling the center via e4. Since both sides have full rook files open, playing e4 would challenge Black's center and allow further coordination.\n",
            "\n",
            "However, note that after 4...cxd5? is not forced yet; rather, we should focus on safe and sound development. In many such setups, Nf3 is preferred because it develops a piece,\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "\n",
            "Scaled Evaluation: 0.046658199374847414 \n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n",
            "\n",
            "----------------------\n",
            "-1.0 reward for illegal syntax\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2132213018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[0;32m--> 648\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "\n",
        "max_prompt_length = 256\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = float(config[\"learning_rate\"]),\n",
        "    adam_beta1 = config[\"adam_beta1\"],\n",
        "    adam_beta2 = config[\"adam_beta2\"],\n",
        "    weight_decay = config[\"weight_decay\"],\n",
        "    warmup_ratio = config[\"warmup_ratio\"],\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    generation_kwargs = {\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        \"repetition_penalty\": 1.2,  # Discourage repetition\n",
        "    },\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = config[\"per_device_train_batch_size\"], #2 for bigger model 4 for smaller #16 gb gpu could do 8 with 14b model\n",
        "    gradient_accumulation_steps = 2, # overall batch size should be 16 or 32 -> sslows training down\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_steps = 10,\n",
        "    max_grad_norm = 0.1,\n",
        "    save_total_limit =1,\n",
        "    report_to = \"wandb\", # report to weights and biases\n",
        "    output_dir = \"./GPRO\",\n",
        "    run_name = \"chess_llama_grpo\",\n",
        ")\n",
        "\n",
        "engine = chess.engine.SimpleEngine.popen_uci(stockfish_path)\n",
        "try:\n",
        "    trainer = GRPOTrainer(\n",
        "        model=model,\n",
        "        processing_class=tokenizer,\n",
        "        reward_funcs=[\n",
        "            xmlcount_reward_func,\n",
        "            soft_format_reward_func,\n",
        "            strict_format_reward_func,\n",
        "            correctness_reward_func,\n",
        "        ],\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "finally:\n",
        "    engine.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "ekXa2Mo25Yu2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9397748ba0214e3d8457571880a931d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24f19c64486e471399961073f6896368",
              "IPY_MODEL_eb07609dfad649a2b7a860124a652ad9",
              "IPY_MODEL_463da6390f414230affebc67dcee8379"
            ],
            "layout": "IPY_MODEL_a5a5481be71f477797dce376440a295e"
          }
        },
        "24f19c64486e471399961073f6896368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_399752160dbd486780f3fe54c7427a8a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4f14cfaf25d74c1b9b99c82042e10f8a",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "eb07609dfad649a2b7a860124a652ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aaff51d7dce43dabaca636c769bf10c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28075cf6c24f457cb1a5bb079c302f5c",
            "value": 2
          }
        },
        "463da6390f414230affebc67dcee8379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c028a1db48e42d496a7280e2a5a57e8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4074d36091314dc7ab7c1b221851583e",
            "value": "â€‡2/2â€‡[00:39&lt;00:00,â€‡18.73s/it]"
          }
        },
        "a5a5481be71f477797dce376440a295e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399752160dbd486780f3fe54c7427a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f14cfaf25d74c1b9b99c82042e10f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1aaff51d7dce43dabaca636c769bf10c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28075cf6c24f457cb1a5bb079c302f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c028a1db48e42d496a7280e2a5a57e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4074d36091314dc7ab7c1b221851583e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}